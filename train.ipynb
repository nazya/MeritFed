{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea397aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from code.train import train\n",
    "from code.optimizers import Optimizer\n",
    "# from code.problems import Problem\n",
    "from code.problem import Loss\n",
    "from code.datasets import Dataset\n",
    "from code.models import Model\n",
    "\n",
    "# %matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb69728-dca1-4d11-a498-69874930542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d681b5-96b4-41e2-86af-563b88775df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_VERBOSE'] = 'True'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = os.path.basename(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429b528b-c7d7-4343-b38f-28fac1783fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MeritFed-M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5fe1472e-477c-42e3-b90c-de97e9c34bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    n_iters: int = 5000\n",
    "    n_peers: int = 20\n",
    "    seed: int = 0\n",
    "\n",
    "    model: Model = field(default_factory=lambda: Model.Linear)\n",
    "    loss: Loss = field(default_factory=lambda: Loss.CrossEntropy)\n",
    "\n",
    "    dataset: Dataset = field(default_factory=lambda: Dataset.MNIST)\n",
    "\n",
    "    \n",
    "    # model: Model = field(default_factory=lambda: Model.Mean)\n",
    "    # loss: Loss = field(default_factory=lambda: Loss.MSE)\n",
    "\n",
    "    # dataset: Dataset = field(default_factory=lambda: Dataset.Normal)\n",
    "    n_samples: int = 500\n",
    "    h_ratio: float = 0.99\n",
    "    mu_normal: float = None\n",
    "\n",
    "    optimizer: Optimizer = field(default_factory=lambda: Optimizer.SGD)\n",
    "    batch_size: int = 10\n",
    "    lr: float = 1e-2\n",
    "\n",
    "    true_weights: bool = None\n",
    "\n",
    "    md_n_iters_: int = None\n",
    "    md_full_: bool = None\n",
    "    md_lr_: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85c250ae-b0a4-4580-a9b3-e84bbf049ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying port 25065\n",
      "test len  500\n",
      "8011\n",
      "full test len  8011\n",
      "full test len  8011\n",
      "calc3 9\n",
      "calc1 438\n",
      "calc2 62\n",
      "actual3 9\n",
      "actual1 438\n",
      "actual2 62\n",
      "CPU times: user 67.7 ms, sys: 69 ms, total: 137 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.optimizer = Optimizer.SGD\n",
    "config.true_weights = True\n",
    "os.environ['MLFLOW_RUN_NAME'] = config.optimizer.name\n",
    "%time train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "468a1261-19c0-4d37-9769-42c9763944fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying port 5393\n",
      "test len  500\n",
      "8011\n",
      "full test len  8011\n",
      "full test len  8011\n",
      "actual3 9\n",
      "calc3 9\n",
      "CPU times: user 115 ms, sys: 79.5 ms, total: 194 ms\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.optimizer = Optimizer.MeritFed\n",
    "config.md_full_ = True\n",
    "config.md_n_iters_ = 20\n",
    "config.md_lr_ = 0.05\n",
    "os.environ['MLFLOW_RUN_NAME'] = config.optimizer.name\n",
    "%time train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d2d62-7fb6-46b4-959c-d86c58b6ed3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad97a582-47d2-485b-92f5-da04d66e9359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying port 10946\n",
      "100\n",
      "CPU times: user 62.1 ms, sys: 72.3 ms, total: 134 ms\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.optimizer = Optimizer.SGD\n",
    "config.true_weights = False\n",
    "os.environ['MLFLOW_RUN_NAME'] = config.optimizer.name\n",
    "%time train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e3591b-3007-45e3-9426-407740cddf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying port 46002\n",
      "2 3\n",
      "3 3\n",
      "650\n",
      "full test len  1000\n",
      "full test len  1000\n",
      "CPU times: user 130 ms, sys: 50.8 ms, total: 181 ms\n",
      "Wall time: 48.1 s\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.optimizer = Optimizer.MeritFed\n",
    "config.md_full_ = False\n",
    "config.md_n_iters_ = 20\n",
    "config.md_lr_ = 0.05\n",
    "os.environ['MLFLOW_RUN_NAME'] = config.optimizer.name\n",
    "%time train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8cc16-e687-405f-a150-c7b56315cf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cf37d84f-9e8c-4022-85d5-3727b399a211",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMNIST\u001b[39;00m(datasets\u001b[38;5;241m.\u001b[39mMNIST):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, rank, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, config, rank, train=True):\n",
    "        root = '/tmp'\n",
    "        self.root = root\n",
    "        if config.n_peers and not rank and self.download:\n",
    "            self.download()\n",
    "\n",
    "        # torch.distributed.barrier()\n",
    "        super().__init__(root=root, train=train, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
    "\n",
    "        self.data, self.targets = self._load_data()\n",
    "        \n",
    "        if train is False:\n",
    "            if rank:\n",
    "                raise RuntimeError(\"Non-master client accessed test dataset\")\n",
    "            mask = self.targets == 1\n",
    "            indices = torch.nonzero(mask).squeeze()\n",
    "            indices = indices[:config.n_samples]\n",
    "            mask = torch.zeros_like(mask).scatter_(0, indices, 1)\n",
    "            self.targets = self.targets[mask].float()\n",
    "            self.data = self.data[mask]\n",
    "            return\n",
    "\n",
    "        self.output_dim = len(self.classes)\n",
    "        self.input_dim = len(self.data[0].view(-1))\n",
    "        # print(self.input_dim)\n",
    "\n",
    "        target_rank_below = 1\n",
    "        near_target_rank_below = 10\n",
    "        self.true_weights = torch.zeros(config.n_peers)\n",
    "        self.true_weights[:target_rank_below] = 1 / target_rank_below\n",
    "        mask = self.targets == 1\n",
    "        if rank < target_rank_below:\n",
    "            indices = torch.nonzero(mask).squeeze()\n",
    "            n = target_rank_below*config.n_samples\n",
    "            if n > len(indices):\n",
    "                raise ValueError('target_rank_below*n_samples too big')\n",
    "            per_worker = config.n_samples\n",
    "            beg = rank * per_worker\n",
    "            end = beg + per_worker\n",
    "            if end > len(indices) - 1:\n",
    "                raise ValueError('invalid partitioning')\n",
    "            indices = indices[beg:end]\n",
    "            # print(beg, end)\n",
    "            # print(indices)\n",
    "            # return dataset\n",
    "        elif target_rank_below <= rank and rank < near_target_rank_below:\n",
    "            target_ratio = config.h_ratio\n",
    "            indices = torch.nonzero(mask).squeeze()\n",
    "            n = target_rank_below*config.n_samples\n",
    "            indices = indices[n:]\n",
    "            if len(indices) < target_ratio*config.n_samples*(near_target_rank_below-target_rank_below):\n",
    "                raise ValueError(f'target_ratio*n_samples*(near_target_rank_below-target_rank_below) too big: {len(indices)} available {target_ratio*config.n_samples*(near_target_rank_below-target_rank_below)} needed')\n",
    "                \n",
    "            per_worker = int(target_ratio*config.n_samples)\n",
    "            beg = (rank-target_rank_below) * per_worker\n",
    "            end = beg + per_worker\n",
    "            # print(end, len(indices))\n",
    "            if end > len(indices) - 1:\n",
    "                raise ValueError('invalid partitioning')\n",
    "            indices = indices[beg:end]\n",
    "\n",
    "            mask = self.targets == 0\n",
    "            for i in range(2, 3):\n",
    "                mask = torch.logical_or(mask, self.targets == i)\n",
    "            more_indices = torch.nonzero(mask).squeeze()\n",
    "            # print((1-target_ratio)*config.n_samples)\n",
    "            per_worker = config.n_samples - int(target_ratio*config.n_samples)\n",
    "            beg = (rank-target_rank_below) * per_worker\n",
    "            end = beg + per_worker\n",
    "            if end > len(more_indices) - 1:\n",
    "                raise ValueError('invalid rounding')\n",
    "            more_indices = more_indices[beg:end]\n",
    "            # print(beg, end)\n",
    "            # print(indices)\n",
    "            # print(len(indices))\n",
    "            # print(more_indices)\n",
    "            # print(len(more_indices))\n",
    "            # print(len(torch.cat((indices, more_indices), 0)))\n",
    "            indices = torch.cat((indices, more_indices), 0)\n",
    "            # print(indices)\n",
    "            # print(len(indices))    \n",
    "            # return dataset\n",
    "        else:\n",
    "            mask = self.targets > 4\n",
    "            indices = torch.nonzero(mask).squeeze()\n",
    "            per_worker = config.n_samples\n",
    "            beg = (rank-target_rank_below) * per_worker\n",
    "            end = min(beg + per_worker, len(indices) - 1)\n",
    "            if end > len(indices) - 1:\n",
    "                raise ValueError('invalid partitioning')\n",
    "            indices = indices[beg:end]\n",
    "            # print(indices)\n",
    "            # print(len(indices))    \n",
    "            # return dataset\n",
    "        # print(indices)\n",
    "        # mask = torch.zeros_like(mask).scatter_(0, indices, 1)\n",
    "        self.targets = self.targets[indices]\n",
    "        print(len(self.targets))\n",
    "        # self.data = self.data[mask]\n",
    "        # if len(self.targets) != config.n_samples:\n",
    "        #         raise ValueError('config failed')\n",
    "\n",
    "    def model_args(self):\n",
    "        return self.input_dim, self.output_dim\n",
    "    \n",
    "    def loss_star(self, full_batch, criterion):\n",
    "        return criterion(full_batch[1], full_batch[1]).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213358de-0f5d-4c0a-ad27-2dd9ee98f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "@dataclass\n",
    "class Config():\n",
    "    n_iters: int = 6000\n",
    "    n_peers: int = 20\n",
    "    seed: int = 0\n",
    "\n",
    "    model: Model = field(default_factory=lambda: Model.Mean)\n",
    "    loss: Loss = field(default_factory=lambda: Loss.MSE)\n",
    "\n",
    "    dataset: Dataset = field(default_factory=lambda: Dataset.Normal)\n",
    "    n_samples: int = 600\n",
    "    h_ratio: float = 0.58\n",
    "    mu_normal: float = None\n",
    "\n",
    "    optimizer: Optimizer = field(default_factory=lambda: Optimizer.SGD)\n",
    "    batch_size: int = 100\n",
    "    lr: float = 1e-2\n",
    "\n",
    "    true_weights: bool = None\n",
    "\n",
    "    md_n_iters_: int = None\n",
    "    md_full_: bool = None\n",
    "    md_lr_: int = None\n",
    "\n",
    "config = Config()\n",
    "for i in range(config.n_peers):\n",
    "    MNIST(config, i, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dc90b-20b3-4dfd-a614-e8fa221dd048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add773e-b43c-4447-ae4d-17dc4578ca44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "vscode": {
   "interpreter": {
    "hash": "18f7a5ae47153a9b42c5447ccb1bbe68959e117ab7750209e163c7c253c9e013"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
