{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea397aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from attrs import define\n",
    "# from code.train import train\n",
    "from code.train_dist import train\n",
    "from code.optimizers.base import Optimizer\n",
    "# from code.problems import Problem\n",
    "from code.problem import Loss\n",
    "from code.datasets import Dataset\n",
    "from code.models import Model\n",
    "\n",
    "# %matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b5d8cd-078e-4292-97c9-0c7a02c57fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zip_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    for instance in zip(*kwargs.values()):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    for instance in itertools.product(*kwargs.values()):\n",
    "        yield dict(zip(keys, instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb69728-dca1-4d11-a498-69874930542c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"AMD\"  # move to train dist\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"TORCH_DEVICE\"] = \"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d681b5-96b4-41e2-86af-563b88775df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_VERBOSE'] = 'True'\n",
    "os.environ['MLFLOW_CHECK_EXIST'] = 'True'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = os.path.basename(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563a0e6-5394-4956-ac72-fef9058294cf",
   "metadata": {},
   "source": [
    "# 4 classes CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279ed62f-ca45-4fa5-9391-85be3895f436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define\n",
    "class BaseConfig():\n",
    "    # nepochs: int = 3\n",
    "    niters:          int = 2500\n",
    "    npeers:          int = 20\n",
    "    seed:            int = 0\n",
    "\n",
    "    loss:           Loss = Loss.CrossEntropyLoss\n",
    "    model:         Model = Model.ResNet18\n",
    "    dataset:     Dataset = Dataset.CIFAR10\n",
    "\n",
    "    nsamples:        int = 1000\n",
    "    hratio:        float = None\n",
    "\n",
    "    optimizer: Optimizer = None\n",
    "    batchsize:       int = 200\n",
    "    lr:            float = 1e-2\n",
    "\n",
    "    trueweights:    bool = None\n",
    "\n",
    "    mdniters_:       int = None\n",
    "    mdlr_:           int = None\n",
    "    mdfull_:        bool = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e9ec-6150-40f7-af5f-bfbf63d313ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.6 ms, sys: 0 ns, total: 43.6 ms\n",
      "Wall time: 42.7 ms\n",
      "CPU times: user 22.6 ms, sys: 15.6 ms, total: 38.2 ms\n",
      "Wall time: 38.2 ms\n",
      "Iterations left: 2448         \r"
     ]
    }
   ],
   "source": [
    "args_grid = dict(\n",
    "    # hratio=[None], \n",
    "    hratio=[0.99, 0.9, 0.7, 0.5],\n",
    "    mdlr_=[0.05],\n",
    "    # 'seed': [0],\n",
    ")\n",
    "\n",
    "for d in product_dict(**args_grid):\n",
    "\n",
    "    os.environ['MLFLOW_RUN_NAME'] = 'SGD Full'\n",
    "    config = BaseConfig(**d)\n",
    "    config.mdlr_ = None\n",
    "    config.optimizer = Optimizer.SGD\n",
    "    config.trueweights = False\n",
    "    # %time train(config)\n",
    "    %time train(config)\n",
    "\n",
    "    os.environ['MLFLOW_RUN_NAME'] = 'SGD Ideal'\n",
    "    config = BaseConfig(**d)\n",
    "    config.mdlr_ = None\n",
    "    config.hratio = None\n",
    "    config.optimizer = Optimizer.SGD\n",
    "    config.trueweights = True\n",
    "    %time train(config)\n",
    "\n",
    "#     os.environ['MLFLOW_RUN_NAME'] = 'MeritFed MD'\n",
    "#     config = BaseConfig(**d)\n",
    "#     config.optimizer = Optimizer.MeritFed\n",
    "#     config.mdfull_ = True\n",
    "#     config.mdniters_ = 20\n",
    "#     # config.md_lr_ = 0.05\n",
    "#     %time train(config)\n",
    "\n",
    "#     os.environ['MLFLOW_RUN_NAME'] = 'MeritFed SMD'\n",
    "#     config = BaseConfig(**d)\n",
    "#     config.optimizer = Optimizer.MeritFed\n",
    "#     config.mdfull_ = False\n",
    "#     config.mdniters_ = 20\n",
    "#     # config.md_lr_ = 0.05\n",
    "#     %time train(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0533c63-5709-4702-96c7-068246fdaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 3/3.. Training Loss: 1.50394,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d705836-cf7c-4c8a-8472-1a709a1e7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random as random\n",
    "import code.datasets\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "root = '/tmp'\n",
    "transform = transforms.ToTensor()\n",
    "# train_data = datasets.CIFAR10(root=root, train=True, download=True, transform=transform)\n",
    "train_data = datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
    "\n",
    "cfg = BaseConfig(hratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac4485-fef2-490d-a421-37f5a356ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split(dset, cfg):\n",
    "\n",
    "#     nclasses = 3\n",
    "#     d = defaultdict(list)\n",
    "#     m = len(dset)\n",
    "#     for i, c in enumerate(dset.classes):\n",
    "#         indices = [j for j, x in enumerate(dset.targets) if x == i]\n",
    "#         indices = np.array(indices)\n",
    "#         np.random.shuffle(indices)\n",
    "#         # d[dset.classes[i]] = indices\n",
    "#         d[i] = indices\n",
    "#         m = min(m, len(indices))\n",
    "\n",
    "#     for i, _ in enumerate(dset.targets):\n",
    "#         dset.targets[i] %= nclasses\n",
    "\n",
    "    \n",
    "#     target_rank_below, near_target_rank_below = 1, 11\n",
    "\n",
    "#     trueweights = torch.zeros(cfg.npeers)\n",
    "#     trueweights[:target_rank_below] = 1 / target_rank_below\n",
    "    \n",
    "#     indices_split = [list() for _ in range(cfg.npeers)]\n",
    "\n",
    "#     hratio = 1.\n",
    "#     m //= 2\n",
    "#     for rank, _ in enumerate(indices_split):\n",
    "#         if rank < target_rank_below:\n",
    "#             for i in range(nclasses):\n",
    "#                 indices_split[rank] += d[i][:m].tolist()\n",
    "#         elif target_rank_below <= rank and rank < near_target_rank_below:\n",
    "#             for i in range(nclasses):\n",
    "#                 n = int(m * cfg.hratio)\n",
    "#                 indices_split[rank] += d[i][m:m+n].tolist()\n",
    "#                 indices_split[rank] += d[i+nclasses][:m-n].tolist()\n",
    "#         else:\n",
    "#             for i in range(nclasses):\n",
    "#                 indices_split[rank] += d[i+2*nclasses][:m].tolist()\n",
    "    \n",
    "#     return indices_split\n",
    "\n",
    "# indices_split = split(train_data, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad40940-9aaf-480d-8764-482323eaef0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random as random\n",
    "def split(dset, npeers, hratio):\n",
    "\n",
    "    nclasses = 3\n",
    "    d = defaultdict(list)\n",
    "    m = len(dset)\n",
    "    for i, c in enumerate(dset.classes):\n",
    "        indices = [j for j, x in enumerate(dset.targets) if x == i]\n",
    "        # indices = np.array(indices)\n",
    "        random.shuffle(indices)\n",
    "        # shuffle(indices)\n",
    "        # d[dset.classes[i]] = indices\n",
    "        d[i] = indices\n",
    "        m = min(m, len(indices))\n",
    "\n",
    "    for i, _ in enumerate(dset.targets):\n",
    "        dset.targets[i] %= nclasses\n",
    "\n",
    "    \n",
    "    target_rank_below, near_target_rank_below = 1, 11\n",
    "\n",
    "    trueweights = torch.zeros(npeers)\n",
    "    trueweights[:target_rank_below] = 1 / target_rank_below\n",
    "    \n",
    "    indices_split = [list() for _ in range(npeers)]\n",
    "\n",
    "    m //= 2\n",
    "    for rank, _ in enumerate(indices_split):\n",
    "        if rank < target_rank_below:\n",
    "            for i in range(nclasses):\n",
    "                indices_split[rank] += d[i][:m]#.tolist()\n",
    "        elif target_rank_below <= rank and rank < near_target_rank_below:\n",
    "            for i in range(nclasses):\n",
    "                n = int(m * hratio)\n",
    "                indices_split[rank] += d[i][m:m+n]#.tolist()\n",
    "                indices_split[rank] += d[i+nclasses][:m-n]#.tolist()\n",
    "        else:\n",
    "            for i in range(nclasses):\n",
    "                indices_split[rank] += d[i+2*nclasses][:m]#.tolist()\n",
    "    for i in indices_split:\n",
    "        print(f\"{i=}\")\n",
    "    return [Subset(dset, inds) for inds in indices_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23799358-c67b-44a2-a6e2-41a8e5e3345c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_kwargs = {'batch_size': config.batchsize,\n",
    "                         'shuffle': True}\n",
    "use_cuda = os.environ[\"TORCH_DEVICE\"] != 'cpu'\n",
    "if use_cuda:\n",
    "    dataloader_kwargs.update({'num_workers': 0,\n",
    "                              'pin_memory': True})\n",
    "\n",
    "\n",
    "\n",
    "train_loaders = dict()\n",
    "train_split = split(train_data, 20, 0.5)\n",
    "for i, inds in enumerate(indices_split):\n",
    "    # print(f\"{inds=}\")\n",
    "    print(f\"{len(inds)=}\")\n",
    "    train_loaders[i] = DataLoader(Subset(dset, inds), **dataloader_kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992635f5-3fb7-4437-9d76-cd209b21405f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = [[i] for i in range(10)]\n",
    "shuffle(x)\n",
    "print(x)\n",
    "\n",
    "# print(x)  gives  [[9], [2], [7], [0], [4], [5], [3], [1], [8], [6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abde4c8-056e-4140-ae80-ea5686fa6075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = [[i] for i in range(10)]\n",
    "random.shuffle(x)\n",
    "print(x)\n",
    "\n",
    "# print(x)  gives  [[9], [2], [7], [0], [4], [5], [3], [1], [8], [6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b230b-e976-4a7e-8b30-6dce08335136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "indices = indices[mdnsamples:]\n",
    "n = len(indices)\n",
    "a = int(np.floor(n / n_workers))\n",
    "top_ind = a * n_workers\n",
    "seq = range(a, top_ind, a)\n",
    "split = np.split(indices[:top_ind], seq)\n",
    "\n",
    "\n",
    "val_data = Subset(train_data, indices=indices[:n_val])\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3888f9-b2fb-40ed-8cf4-a7f034676eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(Subset(dset, d[0]), **dataloader_kwargs)\n",
    "for i in loader:\n",
    "    # print(f\"{i[1]=}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324f6f7-74a6-42ea-ab9f-c848b2e8dc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ebe54-709e-4252-8f1f-e2df126ca140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7704a7-ba5c-440a-b0c5-71b9b3efb57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83e062-7497-49f1-8e12-e01db5eb546c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(dataset_name):\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        train_data = datasets.MNIST(root='data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "        test_data = datasets.MNIST(root='data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "    elif dataset_name == 'cifar10':\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        train_data = datasets.CIFAR10(root='data', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "        \n",
    "        print(f\"{len(train_data.classes)=}\")\n",
    "\n",
    "        test_data = datasets.CIFAR10(root='data', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "    elif dataset_name == 'cifar100':\n",
    "        transform = transforms.ToTensor()  # add extra transforms\n",
    "        train_data = datasets.CIFAR100(root='data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "        test_data = datasets.CIFAR100(root='data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError(dataset_name + ' is not known.')\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_loader_workers, val_loader, test_loader = create_loaders('cifar10', 3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d004701-5bbc-4970-9424-6fc6d0b307bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26da1a-39c1-4457-a2b9-0926e1058175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "vscode": {
   "interpreter": {
    "hash": "18f7a5ae47153a9b42c5447ccb1bbe68959e117ab7750209e163c7c253c9e013"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
